{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Omega$\n",
    "\\begin{equation}\n",
    "\\text{P}(5) = \\frac{1}{6}\n",
    "\\\\\n",
    "\\text{P}(5\\underbrace{\\text{ given the die showed an odd number}}_{\\text{extra notation}}) = \\frac{1}{3}\n",
    "\\\\\n",
    "\\text{P}(5|\\text{odd})\n",
    "\\end{equation}\n",
    "## P(5|odd) is the conditional probability of getting a 5 given the die showed and odd number.\n",
    "***\n",
    "## The number of elements in a set is called the cardinal of the set.\n",
    "## In set notation, cardinal($\\Omega$) is abbreviated as card($\\Omega$).\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{\\text{number of successful outcomes}}{\\text{card(B)}} = \\frac{\\text{card}(A \\cap B)}{\\text{card}(B)} = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation}\n",
    "## Probability of event A happens under the condition B\n",
    "* With P(A $\\cap$ B), we're trying to find the probability of two (A and B), while with P(A|B) we're only trying to find the probability of s single event, which is A.\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B})\n",
    "\\\\\n",
    "P(A \\cap B) = P(A|B) \\cdot P(B)\n",
    "\\\\\n",
    "p(A \\cap B) = P(A) \\cdot P(B|A)\n",
    "\\\\\n",
    "P(A|B) = 1 - P(Ac|B)\n",
    "\\end{equation}\n",
    "***\n",
    "* If event A occurs and probability of B remains unchanged and vice verse (A and B can be any events for any random experiment), then events A and B are said to be **statistically independent.**\n",
    "\n",
    "### Pairwise independent (A and B, A anc C, B and C), mutually independent (A and B and C), A, B, C anre mutually independent only they meet two conditions.\n",
    "\\begin{equation}\n",
    "P(A \\cap B \\cap C) = P(A) \\cdot P(B|A) \\cdot P(C|A \\cap B)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive. If two events are exhaustive, it means they make up the whole sample space $\\Omega$.\n",
    "\\begin{equation}\n",
    "P(A) = P(B \\cap A) + P(B^C \\cap A) = P(B) \\cdot P(A|B) + P(B^C) \\cdot (P(A|B^C)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\overbrace{P(A)}^{P(Delay)} = \\overbrace{P(B_1)}^{P(Boeing)} \\cdot P(A|B_1) + \\overbrace{P(B_2)}^{P(Airbus)} \\cdot P(A|B_2) + \\overbrace{P(B_3)}^{P(ERJ)} \\cdot P(A|B_3)\n",
    "\\end{equation}\n",
    "***\n",
    "# The law of total probability:\n",
    "## Sample space $\\Omega$ is made up n mututally exclusive and exhaustive events:\n",
    "\\begin{equation}\n",
    "\\Omega = \\{B_1, B_2, \\ldots , B_n\\}\n",
    "\\end{equation}\n",
    "## Using the same reasoning, the formula for n event is:\n",
    "\\begin{equation}\n",
    "P(A) = P(B_1) \\cdot P(A|B_1) + P(B_2) \\cdot P(A|B_2) + \\ldots + P(B_n) \\cdot P(A|B_n)\n",
    "\\\\\n",
    "P(A) = \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i)\n",
    "\\end{equation}\n",
    "***\n",
    "# Bayes' theorem\n",
    "\\begin{equation}\n",
    "P(B|A) = \\frac{P(B) \\cdot P(A|B)}{\\displaystyle \\sum_{i=1}^{n} P(B_i) \\cdot P(A|B_i)}\n",
    "\\end{equation}\n",
    "***\n",
    "### Formula:\n",
    "\\begin{aligned}\n",
    "\\text{Conditional Probability} &\\implies P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\\\\n",
    "\\text{The Law of Total Probability} &\\implies P(A) = \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i) \\\\\n",
    "\\text{Bayes' Theorem} &\\implies P(B|A) = \\frac{P(B) \\cdot P(A|B)}{\\displaystyle \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i)}\n",
    "\\end{aligned}\n",
    "## The probability of being infected with HIV before doing any test is called the prior probability. The probability of being infected with HIV after testing positive is called the posterior probability.\n",
    "***\n",
    "## Naive Bayes (simple Bayes or independence Bayes)\n",
    "## Assume conditional independence between $w_1, w_2, w_3, w_4$, conditional independence assumption is central to the Naive Bayes algorithm.\n",
    "\\begin{equation}\n",
    "P(Spam | w_1, w_2,w_3, w_4) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "P(Spam^C | w_1,w_2,w_3,w_4) \\propto P(Spam^C) \\cdot P(w_1|Spam^C) \\cdot P(w_2|Spam^C) \\cdot P(w_3|Spam^C) \\cdot P(w_4|Spam^C) \\\\\n",
    "\\end{equation}\n",
    "## For new message with n words:\n",
    "\\begin{equation}\n",
    "P(Spam | w_1, w_2, \\ldots, w_n) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2 | Spam) \\cdot \\ldots \\cdot P(w_n|Spam)\n",
    "\\\\\n",
    "P(Spam|w_1, w_2, \\ldots, w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\\\P(Spam | w_1,w_2, \\ldots, w_n) \\propto P(Spam) \\cdot \\overbrace{P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot \\ldots \\cdot P(w_n|Spam)}^{\\displaystyle \\prod_{i=1}^{n}P(w_i|Spam)}\n",
    "\\\\\n",
    "P(Spam^C | w_1,w_2, \\ldots, w_n) \\propto P(Spam^C) \\cdot \\prod_{i=1}^{n}P(w_i|Spam^C)\n",
    "\\end{equation}\n",
    "***\n",
    "## Usually, some words from new message appears in spam message but disappear in non-spam message, and vice verse, like\n",
    "\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{\\text{total number of times \"the\" occurs in spam message}}{\\text{total number of words in spam mesages}} = \\frac{0}{7}\n",
    "\\end{equation}\n",
    "## To fix the problem, use a technique called additive smoothing, where we add a smoothing parameter $\\alpha$. (Use $\\alpha$ = 1, $N_{vocabulary}$ represents the number of unique words in all the messages - both spam and non-spam)\n",
    "\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{N_{\\text{\"the\"}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} = \\frac{0+1}{7+1 \\cdot 9}\n",
    "\\end{equation}\n",
    "## To keep the probability values proportional accross all words, we're going to use the additive smoothing for every word.\n",
    "\\begin{equation}\n",
    "P(word|Spam) = \\frac{N_{word|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "## When $\\alpha$ = 1, the additive smoothing technique is most commonly known as Laplace smoothing (or add-one smoothing). \n",
    "## It is also possible to use $\\alpha$ < 1, in which case the technique is called Lidstone smoothing.\n",
    "***\n",
    "# Multinomial Naive Bayes, Gaussian Naive Bayes, Bernoulli Naive Bayes\n",
    "# All the Naive Bayes algorithms build on the (naive) conditional independence assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P(Spam^C | w_1,w_2, ..., w_n) \\propto P(Spam^C) \\cdot \\prod_{i=1}^{n}P(w_i|Spam^C)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P(w_i|Spam^C) = \\frac{N_{w_i|Spam^C} + \\alpha}{N_{Spam^C} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\\begin{aligned}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "It's worth emphasizing that:\n",
    "\n",
    "   * NSpam is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's not equal to the total number of unique words in spam messages.\n",
    "   * NSpamC is equal to the number of words in all the non-spam messages — it's not equal to the number of non-spam messages, and it's not equal to the total number of unique words in non-spam messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
